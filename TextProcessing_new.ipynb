{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from itertools import chain\n",
    "\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from scipy.sparse import dok_matrix\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix, matthews_corrcoef\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB,BernoulliNB\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "#from sklearn import cross_validation\n",
    "from sklearn.model_selection import cross_val_score, train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\"></hr>\n",
    "\n",
    "## Functions and Code Compiled\n",
    "Functions and combined code for the text processing and model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readDataToDataFrame(df):\n",
    "    i = 0\n",
    "    new_df = pd.DataFrame(columns = [\"id\", \"tweet\", \"gender\"])\n",
    "    for k, v in df.items():\n",
    "        filepath = \"data/\"+k+\".xml\"\n",
    "        tree = ET.parse(filepath)\n",
    "        docs = tree.findall('./documents/document')\n",
    "        tweet_text = ' '.join([doc.text for doc in docs])\n",
    "        new_df.loc[i] = [k, tweet_text, v]\n",
    "        i += 1\n",
    "        \n",
    "    gender_encode = {\"gender\":{\"male\":1, \"female\":0}}\n",
    "    new_df.replace(gender_encode, inplace = True)\n",
    "    return new_df\n",
    "\n",
    "def createTrainTestData(data, testdata, train_split = False):\n",
    "    if not train_split:\n",
    "        return data, testdata\n",
    "    np.random.seed(3)\n",
    "    msk = np.random.rand(len(data)) < 0.8\n",
    "    train = data[msk].copy()\n",
    "    test = data[~msk].copy()\n",
    "    return train, test\n",
    "\n",
    "stopwords_list_570 = []\n",
    "with open('./stopwords_en.txt') as f:\n",
    "    stopwords_list_570 = f.read().splitlines()\n",
    "\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl=WordNetLemmatizer()\n",
    "    def __call__(self,doc):\n",
    "        tokenizer = RegexpTokenizer(r\"\\w+\") \n",
    "        tokens = tokenizer.tokenize(doc)\n",
    "        tokenizer = RegexpTokenizer(r\"\\d+\")\n",
    "        number_tokens = tokenizer.tokenize(doc)\n",
    "        \n",
    "        tokens = [w for w in tokens if w not in number_tokens]\n",
    "        rmstopwords = [x for x in tokens if x not in stopwords_list_570]\n",
    "        return [self.wnl.lemmatize(t) for t in rmstopwords]\n",
    "\n",
    "vectorizer=TfidfVectorizer(analyzer='word',input='content',\n",
    "                           lowercase=True,\n",
    "#                            token_pattern='\\w+',\n",
    "                           min_df=0,\n",
    "                           ngram_range=(1,1),\n",
    "                           tokenizer=LemmaTokenizer())\n",
    "\n",
    "# def getDocsLabelsList(train, test, flag = False):\n",
    "#     trainDocs = train.Tweet.tolist()\n",
    "#     trainLabels = train.Gender.tolist()\n",
    "#     testDocs = test.Tweet.tolist()\n",
    "#     if flag:\n",
    "#         testLabels = test.Gender.tolist()\n",
    "#     return trainDocs, trainLabels, testDocs, testLables\n",
    "\n",
    "def showModelStats(models, x_train, y_train, x_test, y_test):\n",
    "#     models = [\n",
    "#     LogisticRegression(),\n",
    "#     BernoulliNB(),\n",
    "#     LinearSVC(),\n",
    "#     RandomForestClassifier()\n",
    "#     ]\n",
    "    for clf in models:\n",
    "        model_name = clf.__class__.__name__\n",
    "        clf.fit(x_train, y_train)\n",
    "        print(model_name)\n",
    "        # Do the prediction\n",
    "        y_predict=clf.predict(x_test)\n",
    "        print(confusion_matrix(y_test,y_predict))\n",
    "        recall=recall_score(y_test,y_predict,average='macro')\n",
    "        precision=precision_score(y_test,y_predict,average='macro')\n",
    "        f1score=f1_score(y_test,y_predict,average='macro')\n",
    "        accuracy=accuracy_score(y_test,y_predict)\n",
    "        matthews = matthews_corrcoef(y_test,y_predict) \n",
    "        print('Accuracy: '+ str(accuracy))\n",
    "        print('Macro Precision: '+ str(precision))\n",
    "        print('Macro Recall: '+ str(recall))\n",
    "        print('Macro F1 score:'+ str(f1score))\n",
    "        print('MCC:'+ str(matthews))\n",
    "\n",
    "def predictValues(models, x_train, y_train, x_test):\n",
    "#     models = [\n",
    "#         LinearSVC(),\n",
    "#     ]\n",
    "    pred_values_dict = {}\n",
    "    for clf in models:\n",
    "        model_name = clf.__class__.__name__\n",
    "        clf.fit(x_train, y_train)\n",
    "        y_predict=clf.predict(x_test)\n",
    "        pred_values_dict[clf.__class__.__name__] = y_predict\n",
    "#     return y_predict\n",
    "    return pred_values_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"train_labels.csv\", index_col = 0, squeeze = True).to_dict()\n",
    "test_data = pd.read_csv(\"test.csv\", index_col = 0, usecols=[\"id\",\"gender\"], squeeze = True).to_dict()\n",
    "\n",
    "train_tweets = readDataToDataFrame(train_data)\n",
    "test_tweets = readDataToDataFrame(test_data)\n",
    "\n",
    "# Set train_split flag to split the training data 80:20\n",
    "train, test = createTrainTestData(train_tweets, test_tweets, train_split = True) \n",
    "\n",
    "trainDocs = train.tweet.tolist()\n",
    "trainLabels = train.gender.tolist()\n",
    "testDocs = test.tweet.tolist()\n",
    "testLabels = test.gender.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(trainDocs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer=CountVectorizer(analyzer='word',input='content',\n",
    "                           lowercase=True,\n",
    "#                            token_pattern='\\w+',\n",
    "                           min_df=0,\n",
    "                           ngram_range=(1,1),\n",
    "                           tokenizer=LemmaTokenizer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and transform training and test docs\n",
    "x_train=vectorizer.fit_transform(trainDocs)\n",
    "y_train=np.asarray(trainLabels)\n",
    "x_test=vectorizer.transform(testDocs)\n",
    "y_test=np.asarray(testLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    LogisticRegression(),\n",
    "    BernoulliNB(),\n",
    "    LinearSVC(),\n",
    "    RandomForestClassifier()\n",
    "    ]\n",
    "\n",
    "showModelStats(models, x_train, y_train, x_test, y_test)\n",
    "pred_dict = predictValues(models, x_train, y_train, x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a list of predicted values from the dictionary by the model name\n",
    "y_pred = pred_dict['LinearSVC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write predicted labels to csv file\n",
    "final_csv = test[['id', 'gender']]\n",
    "# final_csv.rename(columns = {'ID':'id', 'Gender':'gender'}, inplace = True)\n",
    "final_csv.gender = y_pred\n",
    "final_csv.to_csv('pred_labels.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\"> </hr>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
